{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle: Titanic Dataset (Classification Problem)\n",
        "\n",
        "Install kaggle from pypi, upload kaggle.json with api key and build kaggle directory."
      ],
      "metadata": {
        "id": "S5uZ-wVp66Ws"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SC6Zl8WZY2Dk"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files, drive\n",
        "files.upload()\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c titanic\n",
        "!mkdir titanic\n",
        "!unzip titanic.zip -d titanic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download dataset and pip install necessary packages"
      ],
      "metadata": {
        "id": "fdh48lr57K4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autoviz\n",
        "!pip install BorutaShap\n",
        "!pip install featurewiz\n",
        "!pip install auto-sklearn\n",
        "!pip install lazypredict"
      ],
      "metadata": {
        "id": "TUN3scAkTLLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-optimize\n"
      ],
      "metadata": {
        "id": "q_ymKARfw-aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import packages and set matplotlib styling"
      ],
      "metadata": {
        "id": "wkk8nfmF7SRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pandas import Index\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.preprocessing import Normalizer, LabelEncoder, FunctionTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, learning_curve, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import KNNImputer, IterativeImputer, SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pylab as pylab\n",
        "import seaborn as sns\n",
        "from typing import List, Tuple\n",
        "from autoviz import data_cleaning_suggestions, AutoViz_Class\n",
        "from xgboost import XGBClassifier\n",
        "from BorutaShap import BorutaShap\n",
        "from featurewiz import FeatureWiz\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from skopt import BayesSearchCV\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pylab as pylab\n",
        "plt.style.use('seaborn-notebook')\n",
        "plt.rc(\"figure\", autolayout=True)\n",
        "plt.rc(\n",
        "    \"axes\",\n",
        "    labelweight=\"bold\",\n",
        "    labelsize=\"large\",\n",
        "    titleweight=\"bold\",\n",
        "    titlesize=14,\n",
        "    titlepad=10,\n",
        ")\n",
        "sns.set_style('dark')\n",
        "pylab.rcParams['figure.figsize'] = 12, 8\n"
      ],
      "metadata": {
        "id": "aPaRTciuP4do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set our random seed and read our csvs"
      ],
      "metadata": {
        "id": "3qI13KgL7YeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### SET DEFAULTS ###\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "### READ DATA ###\n",
        "train = pd.read_csv('/content/titanic/train.csv')\n",
        "test = pd.read_csv('/content/titanic/test.csv')\n",
        "submission_template = pd.read_csv('/content/titanic/gender_submission.csv')"
      ],
      "metadata": {
        "id": "Mugz-jZFUwh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get data cleaning suggestions\n",
        "\n",
        "Using autoviz package, we can easily get data cleaning suggestions"
      ],
      "metadata": {
        "id": "Vj3u3Wj07cNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### GET DATA CLEANING SUGGESTIONS ###\n",
        "\n",
        "data_cleaning_suggestions(train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "-hKIKETRaAUc",
        "outputId": "fd2d3b96-308a-42ac-dec3-e69e3b94d81b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data cleaning improvement suggestions. Complete them before proceeding to ML modeling.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f384f599e50>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_aa17b_row0_col0, #T_aa17b_row0_col4, #T_aa17b_row1_col0, #T_aa17b_row1_col4, #T_aa17b_row4_col2, #T_aa17b_row4_col3, #T_aa17b_row11_col5 {\n",
              "  background-color: #67000d;\n",
              "  color: #f1f1f1;\n",
              "  font-family: Segoe UI;\n",
              "}\n",
              "#T_aa17b_row0_col1, #T_aa17b_row0_col6, #T_aa17b_row1_col1, #T_aa17b_row1_col6, #T_aa17b_row2_col1, #T_aa17b_row2_col6, #T_aa17b_row3_col1, #T_aa17b_row3_col6, #T_aa17b_row4_col1, #T_aa17b_row4_col6, #T_aa17b_row5_col1, #T_aa17b_row5_col6, #T_aa17b_row6_col1, #T_aa17b_row6_col6, #T_aa17b_row7_col1, #T_aa17b_row7_col6, #T_aa17b_row8_col1, #T_aa17b_row8_col6, #T_aa17b_row9_col1, #T_aa17b_row9_col6, #T_aa17b_row10_col1, #T_aa17b_row10_col6, #T_aa17b_row11_col1, #T_aa17b_row11_col6 {\n",
              "  font-family: Segoe UI;\n",
              "}\n",
              "#T_aa17b_row0_col2, #T_aa17b_row0_col3, #T_aa17b_row0_col5, #T_aa17b_row1_col2, #T_aa17b_row1_col3, #T_aa17b_row1_col5, #T_aa17b_row2_col2, #T_aa17b_row2_col3, #T_aa17b_row2_col5, #T_aa17b_row3_col2, #T_aa17b_row3_col3, #T_aa17b_row3_col5, #T_aa17b_row4_col5, #T_aa17b_row5_col5, #T_aa17b_row6_col2, #T_aa17b_row6_col3, #T_aa17b_row6_col5, #T_aa17b_row7_col2, #T_aa17b_row7_col3, #T_aa17b_row7_col5, #T_aa17b_row8_col0, #T_aa17b_row8_col2, #T_aa17b_row8_col3, #T_aa17b_row8_col4, #T_aa17b_row8_col5, #T_aa17b_row9_col0, #T_aa17b_row9_col2, #T_aa17b_row9_col3, #T_aa17b_row9_col4, #T_aa17b_row10_col0, #T_aa17b_row10_col2, #T_aa17b_row10_col3, #T_aa17b_row10_col4, #T_aa17b_row10_col5, #T_aa17b_row11_col0, #T_aa17b_row11_col2, #T_aa17b_row11_col3, #T_aa17b_row11_col4 {\n",
              "  background-color: #fff5f0;\n",
              "  color: #000000;\n",
              "  font-family: Segoe UI;\n",
              "}\n",
              "#T_aa17b_row2_col0, #T_aa17b_row2_col4 {\n",
              "  background-color: #c7171c;\n",
              "  color: #f1f1f1;\n",
              "  font-family: Segoe UI;\n",
              "}\n",
              "#T_aa17b_row3_col0, #T_aa17b_row3_col4 {\n",
              "  background-color: #fcb398;\n",
              "  color: #000000;\n",
              "  font-family: Segoe UI;\n",
              "}\n",
              "#T_aa17b_row4_col0, #T_aa17b_row4_col4 {\n",
              "  background-color: #fdd5c4;\n",
              "  color: #000000;\n",
              "  font-family: Segoe UI;\n",
              "}\n",
              "#T_aa17b_row5_col0, #T_aa17b_row5_col4 {\n",
              "  background-color: #fee5d9;\n",
              "  color: #000000;\n",
              "  font-family: Segoe UI;\n",
              "}\n",
              "#T_aa17b_row5_col2, #T_aa17b_row5_col3 {\n",
              "  background-color: #fcb99f;\n",
              "  color: #000000;\n",
              "  font-family: Segoe UI;\n",
              "}\n",
              "#T_aa17b_row6_col0, #T_aa17b_row6_col4, #T_aa17b_row7_col0, #T_aa17b_row7_col4 {\n",
              "  background-color: #fff4ef;\n",
              "  color: #000000;\n",
              "  font-family: Segoe UI;\n",
              "}\n",
              "#T_aa17b_row9_col5 {\n",
              "  background-color: #fcbda4;\n",
              "  color: #000000;\n",
              "  font-family: Segoe UI;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_aa17b_\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" >Nuniques</th>\n",
              "      <th class=\"col_heading level0 col1\" >dtype</th>\n",
              "      <th class=\"col_heading level0 col2\" >Nulls</th>\n",
              "      <th class=\"col_heading level0 col3\" >Nullpercent</th>\n",
              "      <th class=\"col_heading level0 col4\" >NuniquePercent</th>\n",
              "      <th class=\"col_heading level0 col5\" >Value counts Min</th>\n",
              "      <th class=\"col_heading level0 col6\" >Data cleaning improvement suggestions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_aa17b_level0_row0\" class=\"row_heading level0 row0\" >PassengerId</th>\n",
              "      <td id=\"T_aa17b_row0_col0\" class=\"data row0 col0\" >891</td>\n",
              "      <td id=\"T_aa17b_row0_col1\" class=\"data row0 col1\" >int64</td>\n",
              "      <td id=\"T_aa17b_row0_col2\" class=\"data row0 col2\" >0</td>\n",
              "      <td id=\"T_aa17b_row0_col3\" class=\"data row0 col3\" >0.00</td>\n",
              "      <td id=\"T_aa17b_row0_col4\" class=\"data row0 col4\" >100.00</td>\n",
              "      <td id=\"T_aa17b_row0_col5\" class=\"data row0 col5\" >0</td>\n",
              "      <td id=\"T_aa17b_row0_col6\" class=\"data row0 col6\" >possible ID column: drop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_aa17b_level0_row1\" class=\"row_heading level0 row1\" >Name</th>\n",
              "      <td id=\"T_aa17b_row1_col0\" class=\"data row1 col0\" >891</td>\n",
              "      <td id=\"T_aa17b_row1_col1\" class=\"data row1 col1\" >object</td>\n",
              "      <td id=\"T_aa17b_row1_col2\" class=\"data row1 col2\" >0</td>\n",
              "      <td id=\"T_aa17b_row1_col3\" class=\"data row1 col3\" >0.00</td>\n",
              "      <td id=\"T_aa17b_row1_col4\" class=\"data row1 col4\" >100.00</td>\n",
              "      <td id=\"T_aa17b_row1_col5\" class=\"data row1 col5\" >1</td>\n",
              "      <td id=\"T_aa17b_row1_col6\" class=\"data row1 col6\" >combine rare categories, possible ID column: drop</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_aa17b_level0_row2\" class=\"row_heading level0 row2\" >Ticket</th>\n",
              "      <td id=\"T_aa17b_row2_col0\" class=\"data row2 col0\" >681</td>\n",
              "      <td id=\"T_aa17b_row2_col1\" class=\"data row2 col1\" >object</td>\n",
              "      <td id=\"T_aa17b_row2_col2\" class=\"data row2 col2\" >0</td>\n",
              "      <td id=\"T_aa17b_row2_col3\" class=\"data row2 col3\" >0.00</td>\n",
              "      <td id=\"T_aa17b_row2_col4\" class=\"data row2 col4\" >76.43</td>\n",
              "      <td id=\"T_aa17b_row2_col5\" class=\"data row2 col5\" >1</td>\n",
              "      <td id=\"T_aa17b_row2_col6\" class=\"data row2 col6\" >combine rare categories</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_aa17b_level0_row3\" class=\"row_heading level0 row3\" >Fare</th>\n",
              "      <td id=\"T_aa17b_row3_col0\" class=\"data row3 col0\" >248</td>\n",
              "      <td id=\"T_aa17b_row3_col1\" class=\"data row3 col1\" >float64</td>\n",
              "      <td id=\"T_aa17b_row3_col2\" class=\"data row3 col2\" >0</td>\n",
              "      <td id=\"T_aa17b_row3_col3\" class=\"data row3 col3\" >0.00</td>\n",
              "      <td id=\"T_aa17b_row3_col4\" class=\"data row3 col4\" >27.83</td>\n",
              "      <td id=\"T_aa17b_row3_col5\" class=\"data row3 col5\" >0</td>\n",
              "      <td id=\"T_aa17b_row3_col6\" class=\"data row3 col6\" >skewed: cap or drop outliers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_aa17b_level0_row4\" class=\"row_heading level0 row4\" >Cabin</th>\n",
              "      <td id=\"T_aa17b_row4_col0\" class=\"data row4 col0\" >147</td>\n",
              "      <td id=\"T_aa17b_row4_col1\" class=\"data row4 col1\" >object</td>\n",
              "      <td id=\"T_aa17b_row4_col2\" class=\"data row4 col2\" >687</td>\n",
              "      <td id=\"T_aa17b_row4_col3\" class=\"data row4 col3\" >77.10</td>\n",
              "      <td id=\"T_aa17b_row4_col4\" class=\"data row4 col4\" >16.50</td>\n",
              "      <td id=\"T_aa17b_row4_col5\" class=\"data row4 col5\" >1</td>\n",
              "      <td id=\"T_aa17b_row4_col6\" class=\"data row4 col6\" >combine rare categories, fill missing, fix mixed data types</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_aa17b_level0_row5\" class=\"row_heading level0 row5\" >Age</th>\n",
              "      <td id=\"T_aa17b_row5_col0\" class=\"data row5 col0\" >88</td>\n",
              "      <td id=\"T_aa17b_row5_col1\" class=\"data row5 col1\" >float64</td>\n",
              "      <td id=\"T_aa17b_row5_col2\" class=\"data row5 col2\" >177</td>\n",
              "      <td id=\"T_aa17b_row5_col3\" class=\"data row5 col3\" >19.87</td>\n",
              "      <td id=\"T_aa17b_row5_col4\" class=\"data row5 col4\" >9.88</td>\n",
              "      <td id=\"T_aa17b_row5_col5\" class=\"data row5 col5\" >0</td>\n",
              "      <td id=\"T_aa17b_row5_col6\" class=\"data row5 col6\" >fill missing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_aa17b_level0_row6\" class=\"row_heading level0 row6\" >SibSp</th>\n",
              "      <td id=\"T_aa17b_row6_col0\" class=\"data row6 col0\" >7</td>\n",
              "      <td id=\"T_aa17b_row6_col1\" class=\"data row6 col1\" >int64</td>\n",
              "      <td id=\"T_aa17b_row6_col2\" class=\"data row6 col2\" >0</td>\n",
              "      <td id=\"T_aa17b_row6_col3\" class=\"data row6 col3\" >0.00</td>\n",
              "      <td id=\"T_aa17b_row6_col4\" class=\"data row6 col4\" >0.79</td>\n",
              "      <td id=\"T_aa17b_row6_col5\" class=\"data row6 col5\" >0</td>\n",
              "      <td id=\"T_aa17b_row6_col6\" class=\"data row6 col6\" ></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_aa17b_level0_row7\" class=\"row_heading level0 row7\" >Parch</th>\n",
              "      <td id=\"T_aa17b_row7_col0\" class=\"data row7 col0\" >7</td>\n",
              "      <td id=\"T_aa17b_row7_col1\" class=\"data row7 col1\" >int64</td>\n",
              "      <td id=\"T_aa17b_row7_col2\" class=\"data row7 col2\" >0</td>\n",
              "      <td id=\"T_aa17b_row7_col3\" class=\"data row7 col3\" >0.00</td>\n",
              "      <td id=\"T_aa17b_row7_col4\" class=\"data row7 col4\" >0.79</td>\n",
              "      <td id=\"T_aa17b_row7_col5\" class=\"data row7 col5\" >0</td>\n",
              "      <td id=\"T_aa17b_row7_col6\" class=\"data row7 col6\" ></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_aa17b_level0_row8\" class=\"row_heading level0 row8\" >Pclass</th>\n",
              "      <td id=\"T_aa17b_row8_col0\" class=\"data row8 col0\" >3</td>\n",
              "      <td id=\"T_aa17b_row8_col1\" class=\"data row8 col1\" >int64</td>\n",
              "      <td id=\"T_aa17b_row8_col2\" class=\"data row8 col2\" >0</td>\n",
              "      <td id=\"T_aa17b_row8_col3\" class=\"data row8 col3\" >0.00</td>\n",
              "      <td id=\"T_aa17b_row8_col4\" class=\"data row8 col4\" >0.34</td>\n",
              "      <td id=\"T_aa17b_row8_col5\" class=\"data row8 col5\" >0</td>\n",
              "      <td id=\"T_aa17b_row8_col6\" class=\"data row8 col6\" ></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_aa17b_level0_row9\" class=\"row_heading level0 row9\" >Embarked</th>\n",
              "      <td id=\"T_aa17b_row9_col0\" class=\"data row9 col0\" >3</td>\n",
              "      <td id=\"T_aa17b_row9_col1\" class=\"data row9 col1\" >object</td>\n",
              "      <td id=\"T_aa17b_row9_col2\" class=\"data row9 col2\" >2</td>\n",
              "      <td id=\"T_aa17b_row9_col3\" class=\"data row9 col3\" >0.22</td>\n",
              "      <td id=\"T_aa17b_row9_col4\" class=\"data row9 col4\" >0.34</td>\n",
              "      <td id=\"T_aa17b_row9_col5\" class=\"data row9 col5\" >77</td>\n",
              "      <td id=\"T_aa17b_row9_col6\" class=\"data row9 col6\" >fill missing, fix mixed data types</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_aa17b_level0_row10\" class=\"row_heading level0 row10\" >Survived</th>\n",
              "      <td id=\"T_aa17b_row10_col0\" class=\"data row10 col0\" >2</td>\n",
              "      <td id=\"T_aa17b_row10_col1\" class=\"data row10 col1\" >int64</td>\n",
              "      <td id=\"T_aa17b_row10_col2\" class=\"data row10 col2\" >0</td>\n",
              "      <td id=\"T_aa17b_row10_col3\" class=\"data row10 col3\" >0.00</td>\n",
              "      <td id=\"T_aa17b_row10_col4\" class=\"data row10 col4\" >0.22</td>\n",
              "      <td id=\"T_aa17b_row10_col5\" class=\"data row10 col5\" >0</td>\n",
              "      <td id=\"T_aa17b_row10_col6\" class=\"data row10 col6\" ></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_aa17b_level0_row11\" class=\"row_heading level0 row11\" >Sex</th>\n",
              "      <td id=\"T_aa17b_row11_col0\" class=\"data row11 col0\" >2</td>\n",
              "      <td id=\"T_aa17b_row11_col1\" class=\"data row11 col1\" >object</td>\n",
              "      <td id=\"T_aa17b_row11_col2\" class=\"data row11 col2\" >0</td>\n",
              "      <td id=\"T_aa17b_row11_col3\" class=\"data row11 col3\" >0.00</td>\n",
              "      <td id=\"T_aa17b_row11_col4\" class=\"data row11 col4\" >0.22</td>\n",
              "      <td id=\"T_aa17b_row11_col5\" class=\"data row11 col5\" >314</td>\n",
              "      <td id=\"T_aa17b_row11_col6\" class=\"data row11 col6\" ></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run AutoViz"
      ],
      "metadata": {
        "id": "pJc8OWzQ7itE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### AUTOVIZ OUR DATA TO GET IDEAS ###\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "dft = AutoViz_Class().AutoViz(\n",
        "    filename=\"\",\n",
        "    depVar=\"Survived\",\n",
        "    dfte=train,\n",
        "    verbose=0,\n",
        ")\n"
      ],
      "metadata": {
        "id": "0PiLXrnFVe3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data wrangling funcs\n",
        "\n",
        "- Drop irrelevant columns (thanks autoviz!)\n",
        "- fill missing data:\n",
        "  - fill categorical features with the mode\n",
        "  - fill numerical features with a prediction made by iterative imputer\n",
        "- Eliminate anomalies\n",
        "  - EllipticEnvelope to detect outliers\n",
        "  - KNNImputer to fill outliers\n",
        "\n",
        "- Wrangle!\n",
        "  - Return the wrangled dataframe and the target variable column if handling the train df\n",
        "  "
      ],
      "metadata": {
        "id": "dVPxyfpD7sAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_cols(df: pd.DataFrame, cols_to_drop: List[str]) -> pd.DataFrame:\n",
        "  print(f'Feature: dropping {cols_to_drop}')\n",
        "  raw = df.copy(deep=True)\n",
        "  raw.drop(labels=cols_to_drop, axis=1, inplace=True)\n",
        "  print(f'Feature: {cols_to_drop} were dropped')\n",
        "  return raw\n",
        "\n",
        "def fill_missing_data(df: pd.DataFrame, cat_features: List[str], num_features: List[str], fill: List[str]) -> pd.DataFrame:\n",
        "  raw = df.copy(deep=True)\n",
        "  for col in fill:\n",
        "    if col in cat_features:\n",
        "        data = raw[col].values.reshape(-1, 1)\n",
        "        raw[col] = SimpleImputer(strategy='most_frequent').fit_transform(data)\n",
        "        print(f'Feature: {col} filled with mode')\n",
        "    if col in num_features:\n",
        "        data = raw[col].values.reshape(-1, 1)\n",
        "        estimator = RandomForestRegressor(n_estimators=4, max_depth=10, \n",
        "                                          bootstrap=True, max_samples=0.5, \n",
        "                                          n_jobs=2, random_state=seed)\n",
        "        raw[col] = IterativeImputer(random_state=seed, estimator=estimator, \n",
        "                                    max_iter=25, tol=1e-1).fit_transform(data)\n",
        "        print(f'Feature: {col} filled with iterative imputer')\n",
        "  return raw\n",
        "\n",
        "def handle_anomalies(df: pd.DataFrame, anomalous_cols: List[str]) -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  Used ellipticenvelope as a suggestion from a peer. I have no idea really how it works!\n",
        "  \"\"\"\n",
        "  raw = df.copy(deep=True)\n",
        "  for col in anomalous_cols:\n",
        "    data = raw[col].values.reshape(-1, 1)\n",
        "    ee = EllipticEnvelope()\n",
        "    knn_imputer = KNNImputer()\n",
        "    outliers_predict = ee.fit(data).predict(data)\n",
        "    raw[col][np.array(np.where(outliers_predict == -1)).ravel()] = np.nan\n",
        "    data = raw[col].values.reshape(-1, 1)\n",
        "    raw[col] = knn_imputer.fit_transform(data)\n",
        "    print(f'Feature: Outliers in {col} eliminated')\n",
        "  return raw\n",
        "\n",
        "def wrangle(df: pd.DataFrame, drops: List[str], fills: List[str], cat_feats: List[str], \n",
        "            num_feats: List[str], anomalies: List[str], target: str, \n",
        "            mode: str='train') -> Tuple[pd.DataFrame, pd.Series]:\n",
        "  raw = df.copy(deep=True)\n",
        "  df_dropped = drop_cols(raw, drops)\n",
        "  print(f'Feature: {drops} were dropped')\n",
        "  df_no_na = fill_missing_data(df_dropped, cat_feats, num_feats, fills)\n",
        "  print(f'Feature: Filled missing data in {cat_feats} and {num_feats}')\n",
        "  df_no_anomalies = handle_anomalies(df_no_na, anomalies)\n",
        "  print(f'Feature: Eliminated outliers in {anomalies}')\n",
        "  target = df_no_anomalies.pop(target) if mode == 'train' else None\n",
        "  print(f'\\n========== {mode} cleaning complete ==========\\n')\n",
        "  return df_no_anomalies, target\n"
      ],
      "metadata": {
        "id": "QgkWYZiVa5qC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wrangling\n",
        "\n",
        "- **Drop**: ID-type data\n",
        "- **Fill**: Age, Embarked, Fare\n",
        "  - These columns have potentially relevant data that we want to impute\n",
        "- **Anomalies**: \n",
        "  - Based on AutoViz, we can see that Fare data has anomalies and appears to be normally distributed, thus we use EllipticEnvelope to detect and fix outliers. Fare data is potentially highly relevant to our prediction\n",
        "- cat_features: categorical features\n",
        "- num_features: discrete and continuous features\n",
        "- target: Survived\n",
        "\n",
        "- wrangle both our X_train and X_test \n"
      ],
      "metadata": {
        "id": "qI_THwW98ySC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "to_drop = [\n",
        "    'PassengerId',\n",
        "    'Ticket'\n",
        "    ]\n",
        "\n",
        "to_fill = [\n",
        "    'Age',\n",
        "    'Embarked',\n",
        "    'Fare'\n",
        "]\n",
        "\n",
        "anomalies = [\n",
        "    'Fare'\n",
        "]\n",
        "\n",
        "cat_features = [\n",
        "    'Embarked',\n",
        "    'Sex'\n",
        "]\n",
        "\n",
        "num_features = [\n",
        "    'Fare',\n",
        "    'Age',\n",
        "    'SibSp',\n",
        "    'Parch',\n",
        "    'Pclass'\n",
        "]\n",
        "\n",
        "# num_features = list(train.select_dtypes(['number']).columns)\n",
        "# cat_features = list(train.select_dtypes(['object']).columns)\n",
        "\n",
        "target = 'Survived'\n",
        "X_train, y_train = wrangle(train, to_drop, to_fill, cat_features, num_features, anomalies, target)\n",
        "X_test, _ = wrangle(test, to_drop, to_fill, cat_features, num_features, anomalies, target, mode='test')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJWQZ7YJfZC3",
        "outputId": "2e1ab322-5e02-4012-9940-7afc207f1728"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature: dropping ['PassengerId', 'Ticket']\n",
            "Feature: ['PassengerId', 'Ticket'] were dropped\n",
            "Feature: ['PassengerId', 'Ticket'] were dropped\n",
            "Feature: Age filled with iterative imputer\n",
            "Feature: Embarked filled with mode\n",
            "Feature: Fare filled with iterative imputer\n",
            "Feature: Filled missing data in ['Embarked', 'Sex'] and ['Fare', 'Age', 'SibSp', 'Parch', 'Pclass']\n",
            "Feature: Outliers in Fare eliminated\n",
            "Feature: Eliminated outliers in ['Fare']\n",
            "\n",
            "========== train cleaning complete ==========\n",
            "\n",
            "Feature: dropping ['PassengerId', 'Ticket']\n",
            "Feature: ['PassengerId', 'Ticket'] were dropped\n",
            "Feature: ['PassengerId', 'Ticket'] were dropped\n",
            "Feature: Age filled with iterative imputer\n",
            "Feature: Embarked filled with mode\n",
            "Feature: Fare filled with iterative imputer\n",
            "Feature: Filled missing data in ['Embarked', 'Sex'] and ['Fare', 'Age', 'SibSp', 'Parch', 'Pclass']\n",
            "Feature: Outliers in Fare eliminated\n",
            "Feature: Eliminated outliers in ['Fare']\n",
            "\n",
            "========== test cleaning complete ==========\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering\n",
        "\n",
        "## Hypothesis: Wealth -> survival\n",
        "\n",
        "### map_cabin:\n",
        "  - Does this passenger have a cabin? \n",
        "\n",
        "### get_family_features:\n",
        "  - How big is this person's family?\n",
        "  - Is this passenger alone?\n",
        "\n",
        "### get_title_features:\n",
        "  - using regex, we extract passenger titles\n",
        "  - Group upper class to 'noble' (we also have a ticket class, but this way we can further separate classes, as some upper class passengers may be much wealthier than others)\n",
        "  - Further group similar titles (i.e. Miss -> Ms)\n",
        "  - Encode these groups of titles\n",
        "\n",
        "### get_bands: \n",
        "- Based on a set of bins, our goal is to create a bunch of integer labels from our continuous variables\n",
        "- create labels using FunctionTransformer on pd.cut, and encode them to int64 type.\n",
        "- Note: Labels are not applied in order (i.e. the bin of 15 < Age <= 30 is mapped to 0) probably due to our transformation. Might make more sense in the future to just use .map()\n",
        "- Age:\n",
        "  - Age is a continuous variable, so we want to turn it into a discrete variable.\n",
        "- Fare:\n",
        "  - Fare is continous as well\n",
        "\n",
        "### get_gender:\n",
        "- Create a gender dummy variable\n",
        "\n",
        "### map_embarked:\n",
        "  - Map the port this passenger embarked from using a label encoder\n",
        "\n",
        "### rename_cols:\n",
        "  - rename remaining columns to fit PEP-style naming conventions\n",
        "\n",
        "### build_features:\n",
        "  - Run all of these functions to build our features\n",
        "  - Normalize if chosen to do so\n"
      ],
      "metadata": {
        "id": "WNcaehI8-Cld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Feature Engineering ###\n",
        "\n",
        "def map_cabin(df: pd.DataFrame):\n",
        "  df['has_cabin'] = df['Cabin'].map(lambda x: 1 - int(type(x) == float))\n",
        "  df.drop(columns=['Cabin'], inplace=True)\n",
        "  print('Feature:  has_cabin dummy variable created, Cabin col dropped')\n",
        "\n",
        "def get_family_features(df: pd.DataFrame):\n",
        "  df['family_size'] = df['SibSp'] + df['Parch'] + 1\n",
        "  df['is_alone'] = 1\n",
        "  df['is_alone'].loc[df['family_size'] > 0] = 0\n",
        "  print('Feature:  family_size and is_alone dummy created')\n",
        "\n",
        "def get_title_features(df: pd.DataFrame):\n",
        "  df['title'] = df['Name'].str.extract('([A-Za-z]+)\\.', expand=False)\n",
        "  df['title'] = df['title'].replace(['Don', 'Rev', 'Dr',\n",
        "       'Major', 'Lady', 'Sir', 'Col', 'Capt', 'Countess',\n",
        "       'Jonkheer'], 'noble')\n",
        "  df['title'] = df['title'].replace('Mlle', 'Ms')\n",
        "  df['title'] = df['title'].replace('Miss', 'Ms')\n",
        "  df['title'] = df['title'].replace('Mme', 'Mrs')\n",
        "  df['title'] = LabelEncoder().fit(df['title'].unique()).transform(df['title'])\n",
        "  df['title'] = df['title'].fillna(0)\n",
        "  df.drop(columns=['Name'], inplace=True)\n",
        "  print('Feature:  title feature created and encoded using LabelEncoder')\n",
        "\n",
        "def get_bands(df: pd.DataFrame, col: str, bins: List[float], labels: List[int]):\n",
        "  if len(bins) - 1 != len(labels):\n",
        "    raise AttributeError('The len of your bins is not equal to the len of your labels')\n",
        "  kwargs = {\n",
        "      'bins': bins,\n",
        "      'labels': labels,\n",
        "      'retbins': False\n",
        "  }\n",
        "  tx = FunctionTransformer(pd.cut, kw_args=kwargs).fit_transform(df[col])\n",
        "  # kbdiscretizer = KBinsDiscretizer(n_bins=5, encode='ordingal') \n",
        "  # would use a kbinsdiscretizer here but wanted to set my own bins for our two continuous variables\n",
        "  df[col.lower()] = LabelEncoder().fit(list(set(tx))).transform(tx)\n",
        "  df.drop(columns=[col], inplace=True)\n",
        "  print(f'Feature:  {col.lower()} feature was created using bands, {col} dropped')\n",
        "\n",
        "def get_gender(df: pd.DataFrame):\n",
        "  df['gender'] = np.where(df['Sex'] == 'male', 1, 0)\n",
        "  df.drop(columns=['Sex'], inplace=True)\n",
        "  print('Feature:  gender dummy created')\n",
        "\n",
        "def map_embarked(df: pd.DataFrame):\n",
        "  df['embarked'] = LabelEncoder().fit(df['Embarked'].unique()).transform(df['Embarked'])\n",
        "  df.drop(columns=['Embarked'], inplace=True)\n",
        "  print(f'Feature:  embarked feature created using LabelEncoder')\n",
        "\n",
        "def rename_cols(df: pd.DataFrame):\n",
        "  df.rename(columns={'Pclass': 'class',\n",
        "             'SibSp': 'sibs_sps',\n",
        "             'Parch': 'par_ch'}, inplace=True)\n",
        "\n",
        "def build_features(df: pd.DataFrame, age_bins: List[float], \n",
        "                   age_labels: List[int], fare_bins: List[float],\n",
        "                   fare_labels: List[int], norm: bool=False):\n",
        "  map_cabin(df)\n",
        "  get_family_features(df)\n",
        "  get_title_features(df)\n",
        "  get_bands(df, 'Age', age_bins, age_labels)\n",
        "  get_bands(df, 'Fare', fare_bins, fare_labels)\n",
        "  get_gender(df)\n",
        "  map_embarked(df)\n",
        "  rename_cols(df)\n",
        "  if norm:\n",
        "    df = pd.DataFrame(StandardScaler().fit_transform(df), columns=df.columns)\n",
        "  print('\\n\\t========== Feature Engineering Completed ==========\\n')\n",
        "  return df"
      ],
      "metadata": {
        "id": "dxmJqjZgiKfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build our new DF with features\n",
        "- Age bins:\n",
        "  - Age is heavily distributed around age 30, which is roughly the mean\n",
        "  - Age also has a short left tail and long right tail\n",
        "\n",
        "- Fare bins:\n",
        "  - Grouped by percentile. Will probably use pd.qcut in the future for this."
      ],
      "metadata": {
        "id": "ZnvlRZLWBzac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "age_bins = [0, 15, 30, 45, 60, np.inf]\n",
        "age_labels = list(range(len(age_bins) - 1))\n",
        "\n",
        "fare_bins = np.percentile(X_train.Fare, [0, 25, 50, 75, 100])\n",
        "fare_labels = list(range(len(fare_bins) - 1))\n",
        "\n",
        "norm = True\n",
        "\n",
        "X_train = build_features(X_train, age_bins, age_labels, fare_bins, fare_labels, norm=norm)\n",
        "X_test = build_features(X_test, age_bins, age_labels, fare_bins, fare_labels, norm=norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iPtPpvTcmX4",
        "outputId": "19e79da3-d4ab-4b1d-aa5e-f006789e5fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature:  has_cabin dummy variable created, Cabin col dropped\n",
            "Feature:  family_size and is_alone dummy created\n",
            "Feature:  title feature created and encoded using LabelEncoder\n",
            "Feature:  age feature was created using bands, Age dropped\n",
            "Feature:  fare feature was created using bands, Fare dropped\n",
            "Feature:  gender dummy created\n",
            "Feature:  embarked feature created using LabelEncoder\n",
            "\n",
            "\t========== Feature Engineering Completed ==========\n",
            "\n",
            "Feature:  has_cabin dummy variable created, Cabin col dropped\n",
            "Feature:  family_size and is_alone dummy created\n",
            "Feature:  title feature created and encoded using LabelEncoder\n",
            "Feature:  age feature was created using bands, Age dropped\n",
            "Feature:  fare feature was created using bands, Fare dropped\n",
            "Feature:  gender dummy created\n",
            "Feature:  embarked feature created using LabelEncoder\n",
            "\n",
            "\t========== Feature Engineering Completed ==========\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection\n",
        "\n"
      ],
      "metadata": {
        "id": "5VrJ83xXfIVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bs_feature_selector = BorutaShap(model=XGBClassifier(),\n",
        "#                               importance_measure='shap',\n",
        "#                               classification=True)\n",
        "# bs_feature_selector.fit(X=X_train, \n",
        "#                         y=y_train,\n",
        "#                         n_trials=500, \n",
        "#                         random_state=seed, \n",
        "#                         verbose=True)\n",
        "# X_train_selected = bs_feature_selector.Subset()\n",
        "# X_test_selected = X_test[bs_feature_selector.accepted]\n",
        "                              \n",
        "features = FeatureWiz(corr_limit=0.7, \n",
        "                      feature_engg='', \n",
        "                      category_encoders='',\n",
        "                      dask_xgboost_flag=True,\n",
        "                      nrows=None,\n",
        "                      verbose=0)\n",
        "X_train_selected = features.fit_transform(X_train, y_train)\n",
        "X_test_selected = features.transform(X_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sAxZ8iw8efh",
        "outputId": "7753edbd-d61c-42ab-e175-ee3ecf7735d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wiz = FeatureWiz(verbose=1)\n",
            "        X_train_selected = wiz.fit_transform(X_train, y_train)\n",
            "        X_test_selected = wiz.transform(X_test)\n",
            "        wiz.features  ### provides a list of selected features ###            \n",
            "        \n",
            "############################################################################################\n",
            "############       F A S T   F E A T U R E  E N G G    A N D    S E L E C T I O N ! ########\n",
            "# Be judicious with featurewiz. Don't use it to create too many un-interpretable features! #\n",
            "############################################################################################\n",
            "Correlation Limit = 0.7\n",
            "Skipping feature engineering since no feature_engg input...\n",
            "Skipping category encoding since no category encoders specified in input...\n",
            "    Since dask_xgboost_flag is True, reducing memory size and loading into dask\n",
            "    Loaded train data. Shape = (891, 12)\n",
            "#### Single_Label Binary_Classification problem ####\n",
            "No test data filename given...\n",
            "#######################################################################################\n",
            "######################## C L A S S I F Y I N G  V A R I A B L E S  ####################\n",
            "#######################################################################################\n",
            "        1 variable(s) to be removed since ID or low-information variables\n",
            "    \tvariables removed = ['is_alone']\n",
            "train data shape before dropping 1 columns = (891, 12)\n",
            "\ttrain data shape after dropping columns = (891, 11)\n",
            "#######################################################################################\n",
            "#####  Searching for Uncorrelated List Of Variables (SULOV) in 10 features ############\n",
            "#######################################################################################\n",
            "    there are no null values in dataset...\n",
            "    Removing (4) highly correlated variables:\n",
            "    ['class', 'par_ch', 'sibs_sps', 'title']\n",
            "    Following (6) vars selected: ['age', 'embarked', 'fare', 'gender', 'family_size', 'has_cabin']\n",
            "Completed SULOV. 6 features selected\n",
            "Time taken for SULOV method = 0 seconds\n",
            "Finally 6 vars selected after SULOV\n",
            "Converting all features to numeric before sending to XGBoost...\n",
            "#######################################################################################\n",
            "#####    R E C U R S I V E   X G B O O S T : F E A T U R E   S E L E C T I O N  #######\n",
            "#######################################################################################\n",
            "Current number of predictors before recursive XGBoost = 6 \n",
            "    Using Dask XGBoost algorithm with 2 virtual CPUs and 13GB memory limit...\n",
            "Dask client configuration: <Client: 'tcp://127.0.0.1:37523' processes=2 threads=2, memory=24.21 GiB>\n",
            "Number of booster rounds = 100\n",
            "Dask XGBoost is crashing due to 'DMatrix' object has no attribute 'worker_map'. Returning with currently selected features...\n",
            "    Completed XGBoost feature selection in 2 seconds\n",
            "#######################################################################################\n",
            "#####          F E A T U R E   S E L E C T I O N   C O M P L E T E D            #######\n",
            "#######################################################################################\n",
            "Selected 6 important features:\n",
            "['age', 'embarked', 'fare', 'gender', 'family_size', 'has_cabin']\n",
            "Total Time taken for featurewiz selection = 4 seconds\n",
            "Output contains a list of 6 important features and a train dataframe\n",
            "    Time taken to create entire pipeline = 4 second(s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SKOPT Search Space\n",
        "\n",
        "We set the search space for our BayesianSearchCV"
      ],
      "metadata": {
        "id": "tOzsVcKH9cKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_space = {\n",
        "        'n_estimators': Integer(100, 1200),\n",
        "        'max_depth': Integer(5, 30),\n",
        "        'min_samples_split': Integer(2, 100),\n",
        "        'min_samples_leaf': Integer(1, 10),\n",
        "        'bootstrap': Categorical([True, False]),\n",
        "        'max_features': Categorical(['log2', 'sqrt'])\n",
        "  }"
      ],
      "metadata": {
        "id": "6RXk1cfZxbjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# skf = StratifiedKFold(n_splits=int(np.floor(1 + np.log2(len(X_train)))))\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=433,\n",
        "                            max_depth=5,\n",
        "                            random_state=42,\n",
        "                            min_samples_leaf=2,\n",
        "                            min_samples_split=18\n",
        "                            )\n",
        "\n",
        "rf.fit(X_, y_train)\n",
        "\n",
        "opt = BayesSearchCV(rf,\n",
        "                    search_space)\n",
        "\n",
        "opt.fit(X_train, y_train)\n",
        "\n",
        "print(opt.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQl5WoWd9brd",
        "outputId": "e9d95ccd-4405-49e9-c6ee-2143a34e79c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8305065595380077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission_template['Survived'] = pred\n",
        "rf.score(X_train_selected, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61hvyvjm65NV",
        "outputId": "8844e161-8942-404e-986f-a3a75f559b5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8338945005611672"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission_template.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "NM1CG3mgCZ04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit -c titanic -f submission.csv -m \"First attempt\"\n",
        "\n",
        "# We got a score of ~0.75"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoY28iGl13cp",
        "outputId": "2e5e3649-ac48-4eac-9f2f-9fb6a0936613"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 4.30k/4.30k [00:02<00:00, 1.68kB/s]\n",
            "Successfully submitted to Titanic - Machine Learning from Disaster"
          ]
        }
      ]
    }
  ]
}